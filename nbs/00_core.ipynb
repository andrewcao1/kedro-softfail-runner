{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\"\"\"``SequentialRunner`` is an ``AbstractRunner`` implementation. It can be\n",
    "used to run the ``Pipeline`` in a sequential manner using a topological sort\n",
    "of provided nodes.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from typing import Any, Dict, Set\n",
    "\n",
    "from pluggy import PluginManager\n",
    "\n",
    "from kedro.io import DataCatalog\n",
    "from kedro.pipeline import Pipeline\n",
    "from kedro.pipeline.node import Node\n",
    "from kedro.runner import SequentialRunner\n",
    "from kedro.runner.runner import run_node\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SoftFailRunner(SequentialRunner):\n",
    "    \"\"\"``SoftFailRunner`` is an ``AbstractRunner`` implementation that runs a\n",
    "    ``Pipeline`` sequentially using a topological sort of provided nodes.\n",
    "    Unlike the SequentialRunner, this runner will not terminate the pipeline\n",
    "    immediately upon encountering node failure. Instead, it will continue to run on\n",
    "    remaining nodes as long as their dependencies are fulfilled.\n",
    "    \"\"\"\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        pipeline: Pipeline,\n",
    "        catalog: DataCatalog,\n",
    "        hook_manager: PluginManager,\n",
    "        session_id: str = None,\n",
    "    ) -> None:\n",
    "        \"\"\"The method implementing sequential pipeline running.\n",
    "\n",
    "        Args:\n",
    "            pipeline: The ``Pipeline`` to run.\n",
    "            catalog: The ``DataCatalog`` from which to fetch data.\n",
    "            hook_manager: The ``PluginManager`` to activate hooks.\n",
    "            session_id: The id of the session.\n",
    "\n",
    "        Raises:\n",
    "            Exception: in case of any downstream node failure.\n",
    "        \"\"\"\n",
    "        nodes = pipeline.nodes\n",
    "        done_nodes = set()\n",
    "        skip_nodes = set()\n",
    "        fail_nodes = set()\n",
    "\n",
    "        load_counts = Counter(chain.from_iterable(n.inputs for n in nodes))\n",
    "        for exec_index, node in enumerate(nodes):\n",
    "            try:\n",
    "                if node in skip_nodes:\n",
    "                    logger.warning(f\"Skipped node: {str(node)}\")\n",
    "                    continue\n",
    "                run_node(node, catalog, hook_manager, self._is_async, session_id)\n",
    "                done_nodes.add(node)\n",
    "            except Exception as e:\n",
    "                new_nodes = self._update_skip_nodes(node, pipeline, skip_nodes)\n",
    "                fail_nodes.add(node)\n",
    "                logger.error(f\"Skipped node: {str(new_nodes)}\")\n",
    "                logger.error(e)\n",
    "            # decrement load counts and release any data sets we've finished with\n",
    "            for data_set in node.inputs:\n",
    "                load_counts[data_set] -= 1\n",
    "                if load_counts[data_set] < 1 and data_set not in pipeline.inputs():\n",
    "                    catalog.release(data_set)\n",
    "            for data_set in node.outputs:\n",
    "                if load_counts[data_set] < 1 and data_set not in pipeline.outputs():\n",
    "                    catalog.release(data_set)\n",
    "\n",
    "            logger.info(\n",
    "                \"Completed %d out of %d tasks\",\n",
    "                len(done_nodes),\n",
    "                len(nodes),\n",
    "            )\n",
    "        self._summary(pipeline, skip_nodes, fail_nodes)\n",
    "\n",
    "        if skip_nodes:\n",
    "            self._suggest_resume_scenario(pipeline, done_nodes)\n",
    "\n",
    "    def _update_skip_nodes(self, node, pipeline, skip_nodes=None) -> Set[Node]:\n",
    "        \"\"\"Traverse the DAG with Breath-First-Search (BFS) to find all descendent nodes.\n",
    "        `skip_nodes` is used to eliminate unnecessary search path, the `skip_nodes` will be\n",
    "        updated during the search.\n",
    "\n",
    "        Args:\n",
    "        node: A ``Node`` that need to be skipped due to exception.\n",
    "        node_dependencies: Node dependencies Dict[Node, Set[Node]], the key is the Node\n",
    "        and the value is the child of the node.\n",
    "        skip_nodes: A set of Node to be skipped.\n",
    "\n",
    "        Returns:\n",
    "            The set of nodes that need to be skipped.\n",
    "        \"\"\"\n",
    "        node_set = pipeline.from_nodes(node.name).nodes\n",
    "        skip_nodes |= set(pipeline.from_nodes(node.name).nodes)\n",
    "        return node_set\n",
    "\n",
    "    def _summary(self, pipeline, skip_nodes, fail_nodes):\n",
    "        if fail_nodes:\n",
    "            fail_nodes_names = [node.name for node in fail_nodes]\n",
    "            logger.warning(\"These are the nodes with error %s\", fail_nodes_names)\n",
    "        if skip_nodes:\n",
    "            skip_nodes_names = [node.name for node in skip_nodes]\n",
    "            skipped_output = pipeline.only_nodes(*skip_nodes_names).outputs()\n",
    "            logger.warning(\n",
    "                \"These are the datasets that haven't been generated %s\", skipped_output\n",
    "            )\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        pipeline: Pipeline,\n",
    "        catalog: DataCatalog,\n",
    "        hook_manager: PluginManager = None,\n",
    "        session_id: str = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Run the ``Pipeline`` using the datasets provided by ``catalog``\n",
    "        and save results back to the same objects.\n",
    "\n",
    "        Args:\n",
    "            pipeline: The ``Pipeline`` to run.\n",
    "            catalog: The ``DataCatalog`` from which to fetch data.\n",
    "            hook_manager: The ``PluginManager`` to activate hooks.\n",
    "            session_id: The id of the session.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Raised when ``Pipeline`` inputs cannot be satisfied.\n",
    "\n",
    "        Returns:\n",
    "            Any node outputs that cannot be processed by the ``DataCatalog``.\n",
    "            These are returned in a dictionary, where the keys are defined\n",
    "            by the node outputs.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        hook_manager = hook_manager or _NullPluginManager()\n",
    "        catalog = catalog.shallow_copy()\n",
    "\n",
    "        unsatisfied = pipeline.inputs() - set(catalog.list())\n",
    "        if unsatisfied:\n",
    "            raise ValueError(\n",
    "                f\"Pipeline input(s) {unsatisfied} not found in the DataCatalog\"\n",
    "            )\n",
    "\n",
    "        free_outputs = pipeline.outputs() - set(catalog.list())\n",
    "        unregistered_ds = pipeline.data_sets() - set(catalog.list())\n",
    "        for ds_name in unregistered_ds:\n",
    "            catalog.add(ds_name, self.create_default_data_set(ds_name))\n",
    "\n",
    "        if self._is_async:\n",
    "            self._logger.info(\n",
    "                \"Asynchronous mode is enabled for loading and saving data\"\n",
    "            )\n",
    "        self._run(pipeline, catalog, hook_manager, session_id)\n",
    "\n",
    "        # self._logger.warn(\"Pipeline execution completed successfully.\")\n",
    "\n",
    "        # Override runner temporarily - need to handle the GC properly, not important for now\n",
    "        # run_result = {}\n",
    "        # for ds_name in free_outputs:\n",
    "        #     try:\n",
    "        #         run_result[ds_name] = catalog.load(ds_name)\n",
    "        #     except DataSetError:\n",
    "        #         # Due to some nodes are skipped, the GC is not behave correctly as some\n",
    "        #         # data haven't been loaded\n",
    "        #         continue\n",
    "        # return run_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
